---
layout: post
title:  "Ignore Testing other formats"
date:   2025-03-25 20:09:29 -0600
categories: databases, work, sharding, replication
---

# Lessons from Scaling Elasticsearch the Wrong Way ğŸš¨

## Introduction
Picture this: You're building a B2B e-commerce platform (think Shopify, but smaller and simpler), and you need a solid search function for all those products. Enter **Elasticsearch**â€”fast, powerful, andâ€¦ deceptively easy to scale **the wrong way**.

I was a backend engineer on a small team of three, and we thought we were following best practices. But instead of scaling for success, we scaled straight into **self-inflicted chaos**.

This is the story of how we turned our Elasticsearch cluster into a ticking time bombâ€”and what we learned when it finally exploded.

## What is Elasticsearch? (A Crash Course)
For the uninitiated, Elasticsearch is a document-based database optimized for full-text search, backed by Apache Lucene. Instead of tables (like in MySQL) or collections (like MongoDB), it uses indexes, and inside those indexes, you store JSON documents.

Example document:

json
Copy
Edit
{
  "product_id": 12345,
  "name": "Cordless Drill",
  "category": "Electronics",
  "price": 69.99,
  "stock": 50,
  "description": "An electric drill which uses rechargeable batteries"
}
Our setup? We hosted each brandâ€™s products in its own Elasticsearch index, meaning every company got its own index.

At first, it seemed like a great idea. But little did we know, we were on a one-way path to disaster.

Scaling Upâ€¦ And Digging Our Own Grave
We started with a single-node Elasticsearch cluster on AWS OpenSearch (formerly AWS Elasticsearch). But as more brands uploaded products, we needed more storage.

So, we followed the textbook scale-up approach:

Bigger machines âœ…

Three master nodes (to avoid split-brain issues) âœ…

Read replicas for faster queries âœ…

Primary â†’ Replica writes (CQRS hype!) âœ…

Everything was greatâ€”until it wasnâ€™t.

Then We Hit a Hard Limitâ€¦
One day, our async process failed to write new indexes. Why?

ğŸ‘‰ AWS OpenSearch has a limit of 1,000 indexes per node.

"No problem," we thought. "We'll just add another node." (Famous last words.)

Enter: Sharding & Replicationâ€”Our Unintentional Villains
Elasticsearch distributes data using shards (splitting data across nodes) and replicas (backup copies for redundancy).

Each index is made up of multiple shards.

AWS automatically creates at least one replica per primary shard.

So, every time we added a new brand:
âœ… New index â†’ âœ… New primary shard â†’ âœ… New replica shard â†’ âŒ More memory & file handle overhead

Multiply that across hundreds of brandsâ€¦ and our cluster started suffocating.

How Bad Did It Get?
We kept adding nodesâ€”but instead of solving the problem, we were just delaying the inevitable.

Cluster ballooned to ğŸš€ 3 master nodes + 5 high-memory nodes

Total product data? A few GBs at most

Total shards? THOUSANDS (each eating memory & resources)

Query performance? Degrading fast ğŸ’€

Uploads failing? Yep ğŸ˜µ

Elasticsearch recommends shard sizes between 10GB - 50GB.
ğŸ‘‰ Ours? Mostly under 1GB each.

We had created too many tiny shardsâ€”and it was killing our cluster.

The Breaking Point
At our peak cluster madness:

AWS OpenSearch had a hard shard limit (~2000 shards per domain).

Our memory was maxed out trying to manage thousands of small indexes.

Deleting old indexes manually just to keep things running.

A failed Elasticsearch version upgrade left our cluster in a degraded state.

At this point, we had two choices:

ğŸ›‘ Keep scaling (and burn $$$) OR
âœ… Fix our indexing strategy once and for all.

How We Fixed It (And What You Should Do Instead)
Instead of one index per brand, we switched to a single consolidated index with a field for brand_id.

Scaling the Right Way: How Many Shards Do You Actually Need?
Data Size	Suggested Shards	Suggested Replicas
< 10GB	1-3 shards	1 replica
10GB-100GB	5-10 shards	1-2 replicas
> 100GB	10+ shards	2+ replicas
ğŸš€ Result?

âœ… Memory usage dropped drastically
âœ… Cluster performance improved instantly
âœ… New uploads stopped failing
âœ… No more manual index deletions
âœ… We saved a ton of $$$ on AWS costs

Key Takeaways for Scaling Elasticsearch Properly
ğŸ”¹ Avoid too many small indexes â†’ Consolidate when possible.
ğŸ”¹ Shards consume memory, even if theyâ€™re tiny â†’ Fewer, larger shards are better.
ğŸ”¹ Plan your indexing strategy based on data size â†’ Not arbitrary business rules.
ğŸ”¹ Read the AWS OpenSearch fine print â†’ Hard limits exist, and they hurt.
ğŸ”¹ Scaling out â‰  Scaling right â†’ More nodes donâ€™t always fix bad design.

Big shoutout to AWS engineers who helped us troubleshoot this mess. (Yes, we even managed to break things during an upgrade. ğŸ˜…)

Final Thoughts
Was this Elasticsearchâ€™s fault? Nope.
Was this AWS OpenSearchâ€™s fault? Also nope.
Was this our fault for designing it poorly? 100%.

Lesson learned: Scaling Elasticsearch is easy. Scaling it correctly is hard.

So, the next time you're designing an Elasticsearch clusterâ€”think before you shard. ğŸ˜‰